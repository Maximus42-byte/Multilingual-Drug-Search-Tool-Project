# -*- coding: utf-8 -*-
"""Final_NLP_HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OGZdmII1XPQfCnpKb4lcun12yQT3xAkm
"""

import numpy as np
import pandas as pd
import re

drugs = pd.read_csv('/content/Untitled spreadsheet - drugs.csv')

r_sign = r"®"
r_sign_reg = f"{r_sign}"

english = r"[a-zA-Z]"
english_reg = f"{english}"

BULK= r"\([^\w]*ت[^\w]*\)"
BULK2= r"\([^\w]*ت"
BULK_reg = f"{BULK}"
BULK2_reg = f"{BULK2}"

percent= r"%"
percent_reg = f"{percent}"

num = r"[0-9]*"
num_reg = f"{num}"

quan0=r".*(ML|ml|MG|mg|GR|gr|g|cc|CC|G|mili|MILI|MIL|mil).*"
quan0_reg = f"{quan0}"

quan=r"[^\w](ML|ml|MG|mg|GR|CC|IU)"
quan_reg = f"{quan}"
quan2=r"[^\w](ML|ml|MG|mg|GR|gr|g|cc|CC|G|mili|MILI|MIL|mil|IU|U|MCG|T)[^\w]"
quan2_reg = f"{quan2}"

ml=r".*(ML|ml).*"
ml_reg = f"{ml}"
# ml2= r"[^\w](م|سی سی|سي سي|میل|میلی|ميلي|میلی لیتری|م ل)"
# ml2_reg = f"{ml2}"
ml4= r"[^\w](م|سی سی|سي سي|میل|میلی|ميلي|میلی لیتری|م ل)[^\w]"
ml4_reg = f"{ml4}"
ml3= r".*(ميلي ليتر|میلی لیتر).*"
ml3_reg = f"{ml3}"

mg0=r"(MG|mg)"
mg_reg0 = f"{mg0}"

mg=r".*(MG|mg).*"
mg_reg = f"{mg}"
# mg2= r"[^\w](م|سی سی|سي سي|میل|میلی|ميلي|میلی گرمی|م گ)"
# mg2_reg = f"{mg2}"
mg4= r"[^\w](م|سی سی|سي سي|میل|میلی|ميلي|میلی گرمی|م گ)[^\w]"
mg4_reg = f"{mg4}"
mg3= r".*(ميلي گرم|میلی گرم).*"
mg3_reg = f"{mg3}"

gr0=r"(GR|gr|g)"
gr_reg0 = f"{gr0}"

gr=r".*(GR|gr).*"
gr_reg = f"{gr}"
gr2= r"[^\w](g|گ|م)[^\w]"
gr4= r"[^\w](g|گ|م)"
gr2_reg = f"{gr2}"
gr4_reg = f"{gr4}"
gr3= r".*(گرم).*"
gr3_reg = f"{gr3}"

parantese1 = r"\("
parantese1_reg = f"{parantese1}"
parantese2 = r"\)"
parantese2_reg = f"{parantese2}" 

quantity =r".*(میل|سی سی|سي سي|\W+م\W+|ميلي|گرم|ميکرو گرم|گرمي|عددي|واحدي|يکرو|\W+گ\W+|میکرو).*"
quantity_reg = f"{quantity}" 
quantity2 =r"(سی سی|سي سي|ميلي|ميکرو گرم|گرمي|عددي|واحدي|ميکروگرم|درصد)"
quantity2_reg = f"{quantity2}"
quantity3 =r"(ميل|گرم|میکرو)"
quantity3_reg = f"{quantity3}"  
quantity4 =r"(\W+م\W+|\W+گ\W+|\W+ع\W+)"
quantity4_reg = f"{quantity4}" 
quantity5 =r"\b.*\W+م\b"
quantity5_reg = f"{quantity5}"
quantity6 =r"\b.*\W+ع\b"
quantity6_reg = f"{quantity6}"  

num0 = r".*[0-9|/|%|+|-|_|-|.|-|.]+.*"
num0_reg = f"{num0}" 
num = r"[0-9|/|%|+|-|_|-|.|-|.|<|>|-|-|,|`|!|$|&]+"
num_reg = f"{num}" 

eng = r"[a-zA-Z]+"
eng_reg = f"{eng}" 
white = r"\W+"
white_reg = f"{white}" 

hash = r"هاش"
hash_reg = f"{hash}" 
 
# stri =  "کلوگزاسيلين کپسول 500  ميلي گرم"
# print(not(re.match(ml3_reg, stri)))
# print(not(re.match(mg3_reg, stri)))
# s= re.split('[0-9]+',stri)
# indexy = []

# print(stri)
# stri = re.sub(percent_reg, ' درصد ', stri)
# print(s)

drug_clean = pd.DataFrame([], columns=drugs.columns)

counter = 0
for i in range(len(drugs)):
  ###################         persian         #####################
  if(len(str(drugs["فارسی"].iloc[i])) <= 5 or len(str(drugs["english"].iloc[i])) <= 5 or not(re.match(english_reg, drugs["english"].iloc[i]))):
    
    # drugs.drop(i, axis=0, inplace=True)
    continue
  # print(drugs["فارسی"].iloc[i])
  # print(drugs["english"].iloc[i])
  
  indexy = []
  for j in range(len(drugs["فارسی"].iloc[i])-1):
    first = drugs["فارسی"].iloc[i][j]
    sec = drugs["فارسی"].iloc[i][j+1]
    if( first >= '0' and first <='9'  and  (sec < '0' or sec > '9') and sec!= "." and sec!= "/" and sec!= "%" ):
      indexy.append(j+1+len(indexy))
  # indexy2 = []
  # for j in range(len(drugs["فارسی"].iloc[i])-1):
  #   first = drugs["فارسی"].iloc[i][j]
  #   sec = drugs["فارسی"].iloc[i][j+1]
  #   if( sec >= '0' and sec <='9'  and  (first < '0' or first > '9') and first!= "." and first!= "/" and first!= "%" ):
  #     indexy2.append(j+1+len(indexy2))
   
  for j in range(len(indexy)):
    drugs["فارسی"].iloc[i] = drugs["فارسی"].iloc[i][:indexy[j]] + " " + drugs["فارسی"].iloc[i][indexy[j]:]
   
  
  drugs["فارسی"].iloc[i] = re.sub(BULK_reg, ' فله ', drugs["فارسی"].iloc[i])
  drugs["فارسی"].iloc[i] = re.sub(BULK2_reg, ' فله ', drugs["فارسی"].iloc[i])
  drugs["فارسی"].iloc[i] = re.sub(percent_reg, ' ', drugs["فارسی"].iloc[i])
  if(re.match(ml3_reg, drugs["فارسی"].iloc[i]) or re.match(mg3_reg, drugs["فارسی"].iloc[i]) or re.match(gr3_reg, drugs["فارسی"].iloc[i])):
    y=0
  elif (re.match(ml_reg, drugs["english"].iloc[i])):
    # drugs["فارسی"].iloc[i] = re.sub(ml2_reg, 'میلی لیتر ', drugs["فارسی"].iloc[i])  
    drugs["فارسی"].iloc[i] = re.sub(ml4_reg, ' ', drugs["فارسی"].iloc[i])  
  elif (re.match(mg_reg, drugs["english"].iloc[i])):
    # drugs["فارسی"].iloc[i] = re.sub(mg2_reg, 'میلی گرم ', drugs["فارسی"].iloc[i])
    drugs["فارسی"].iloc[i] = re.sub(mg4_reg, ' ', drugs["فارسی"].iloc[i])
  elif (re.match(gr_reg, drugs["english"].iloc[i])):
    drugs["فارسی"].iloc[i] = re.sub(gr2_reg, ' ', drugs["فارسی"].iloc[i])
    drugs["فارسی"].iloc[i] = re.sub(gr4_reg, ' ', drugs["فارسی"].iloc[i])

    

  drugs["فارسی"].iloc[i] = re.sub(parantese1_reg, ' ', drugs["فارسی"].iloc[i]) 
  drugs["فارسی"].iloc[i] = re.sub(parantese2_reg, ' ', drugs["فارسی"].iloc[i])
  # if(re.match(quantity_reg, drugs["فارسی"].iloc[i])):
  #   print(drugs["فارسی"].iloc[i])
  drugs["فارسی"].iloc[i] = re.sub(quantity2_reg, ' ', drugs["فارسی"].iloc[i]) 
  drugs["فارسی"].iloc[i] = re.sub(quantity3_reg, ' ', drugs["فارسی"].iloc[i])
  drugs["فارسی"].iloc[i] = re.sub(quantity4_reg, ' ', drugs["فارسی"].iloc[i])
    # drugs["فارسی"].iloc[i] = re.sub(quantity5_reg, ' ', drugs["فارسی"].iloc[i])
    # print(drugs["فارسی"].iloc[i])
    # print("*************************************************")
  if(re.match(quantity5_reg, drugs["فارسی"].iloc[i])and drugs["فارسی"].iloc[i][len(drugs["فارسی"].iloc[i])-1] == 'م'):
    # print(drugs["فارسی"].iloc[i])
    # print(drugs["فارسی"].iloc[i][len(drugs["فارسی"].iloc[i])-1])
    # drugs["فارسی"].iloc[i] = re.sub(quantity5_reg, ' ', drugs["فارسی"].iloc[i])
    drugs["فارسی"].iloc[i] = drugs["فارسی"].iloc[i][:len(drugs["فارسی"].iloc[i])-2]
    # print(drugs["فارسی"].iloc[i])
    # print("*************************************************")
  if(re.match(quantity6_reg, drugs["فارسی"].iloc[i])and drugs["فارسی"].iloc[i][len(drugs["فارسی"].iloc[i])-1] == 'ع'):
    # print(drugs["فارسی"].iloc[i])
    # print(drugs["فارسی"].iloc[i][len(drugs["فارسی"].iloc[i])-1])
    # drugs["فارسی"].iloc[i] = re.sub(quantity5_reg, ' ', drugs["فارسی"].iloc[i])
    drugs["فارسی"].iloc[i] = drugs["فارسی"].iloc[i][:len(drugs["فارسی"].iloc[i])-2]
    # print(drugs["فارسی"].iloc[i])
    # print("*************************************************")
  # if(re.match(num0_reg, drugs["فارسی"].iloc[i]) ):
  #   print(drugs["فارسی"].iloc[i])
  drugs["فارسی"].iloc[i] = re.sub(num_reg, ' ', drugs["فارسی"].iloc[i]) 
    
    # print(drugs["فارسی"].iloc[i])
    # print("*************************************************")
  

  drugs["فارسی"].iloc[i] = re.sub(eng_reg, ' ', drugs["فارسی"].iloc[i])
  drugs["فارسی"].iloc[i] = re.sub(white_reg, ' ', drugs["فارسی"].iloc[i])
  # print(drugs["فارسی"].iloc[i])
  
  counter+=1

  ###################         english         #####################
  drugs["english"].iloc[i] = re.sub(parantese1_reg, ' ', drugs["english"].iloc[i]) 
  drugs["english"].iloc[i] = re.sub(parantese2_reg, ' ', drugs["english"].iloc[i])
  drugs["english"].iloc[i] = re.sub(num_reg, ' ', drugs["english"].iloc[i]) 
  drugs["english"].iloc[i] = re.sub(percent_reg, ' ', drugs["english"].iloc[i])
  drugs["english"].iloc[i] = re.sub(r_sign_reg, ' ', drugs["english"].iloc[i])
  # if(re.match(quan0_reg, drugs["english"].iloc[i])):
  #   x1 = drugs["english"].iloc[i]
  if(re.match(hash_reg, drugs["فارسی"].iloc[i])):
    x=1
  else:
    drugs["english"].iloc[i] = re.sub(quan_reg, ' ', drugs["english"].iloc[i])
    drugs["english"].iloc[i] = re.sub(quan2_reg, ' ', drugs["english"].iloc[i])
    # if(len(x1) != len(drugs["english"].iloc[i])):
    #   print(x1)
    #   print(drugs["english"].iloc[i])
    #   print("*************************************************")
  drugs["english"].iloc[i] = drugs["english"].iloc[i].lower()
  drug_clean = drug_clean.append(drugs.iloc[i], ignore_index = True)


# print(counter)

#cleand data
drug_clean

import torch
import numpy as np
import pandas as pd
from torch.autograd import Variable
from torch.utils.data import DataLoader
from sklearn import decomposition
import seaborn as sns

def prepare_set_ravel(corpus, n_gram = 3):
    columns = ['Input', 'Output']
    result = pd.DataFrame(columns = columns)
    for sentence in corpus:
        for i,w in enumerate(sentence.split()):
            inp = w
            for n in range(1,n_gram+1):
                # look back
                if (i-n)>=0:
                    out = sentence.split()[i-n]
                    row = pd.DataFrame([[inp,out]], columns = columns)
                    result = result.append(row, ignore_index = True)
                
                # look forward
                if (i+n)<len(sentence.split()):
                    out = sentence.split()[i+n]
                    row = pd.DataFrame([[inp,out]], columns = columns)
                    result = result.append(row, ignore_index = True)
    return result

def create_vocabulary(corpus):
    vocabulary = {}
    i = 0
    for s in corpus:
        for w in s.split():
            if w not in vocabulary:
                vocabulary[w] = i
                i+=1
    return vocabulary

english_corpus = drug_clean['english']

persian_corpus = drug_clean['فارسی']

print(len(english_corpus))
print(len(persian_corpus))

english_vocabulary = create_vocabulary(english_corpus)
english_vocabulary

persian_vocabulary = create_vocabulary(persian_corpus)
persian_vocabulary

english_train_emb = prepare_set_ravel(english_corpus, n_gram=3)
print(english_train_emb.shape)
english_train_emb.head()

english_train_emb[:-100]

persian_train_emb = prepare_set_ravel(persian_corpus, n_gram=3)
print(persian_train_emb.shape)
persian_train_emb.tail()

persian_train_emb[:-100]

english_train_emb.Input = english_train_emb.Input.map(english_vocabulary)
english_train_emb.Output = english_train_emb.Output.map(english_vocabulary)
english_train_emb.head()

persian_train_emb.Input = persian_train_emb.Input.map(persian_vocabulary)
persian_train_emb.Output = persian_train_emb.Output.map(persian_vocabulary)
persian_train_emb.head()

english_vocab_size = len(english_vocabulary)
persian_vocab_size = len(persian_vocabulary)

print(persian_vocab_size)
print(english_vocab_size)

class SkipGram():
  def __init__(self, embedding_dims, vocab_size, model_name, num_epochs=5000, batch_size=1000, learning_rate=1, lr_decay=0.99):
    self.embedding_dims = embedding_dims
    self.device = torch.device('cuda')
    self.vocab_size = vocab_size
    initrange = 0.5 / embedding_dims
    self.W1 = Variable(torch.randn(vocab_size, embedding_dims, device=self.device).uniform_(-initrange, initrange).float(), requires_grad=True)
    self.W2 = Variable(torch.randn(embedding_dims, vocab_size, device=self.device).uniform_(-initrange, initrange).float(), requires_grad=True)
    self.num_epochs = num_epochs
    self.batch_size = batch_size
    self.learning_rate = learning_rate
    self.lr_decay = lr_decay
    self.loss_hist = []
    self.layer1_params_file = f"{model_name}_layer1_params.pt"
    self.layer2_params_file = f"{model_name}_layer2_params.pt"
  
  def get_input_tensor(self, tensor):
    tensor = tensor.to(self.device)
    size = [*tensor.shape][0]
    inp = (torch.zeros(size, self.vocab_size).to(self.device).scatter_(1, tensor.unsqueeze(1).to(self.device), 1.)).to(self.device)
    
    return Variable(inp).float()
  

  def fit(self, train_emb):
    for epoch in range(self.num_epochs):
      for x,y in zip(DataLoader(train_emb.Input.values, batch_size=self.batch_size), DataLoader(train_emb.Output.values, batch_size=self.batch_size)):
          
          input_tensor = self.get_input_tensor(x).to(self.device)
      
          h = input_tensor.mm(self.W1).to(self.device)
          y_pred = h.mm(self.W2).to(self.device)
          
          loss_f = torch.nn.CrossEntropyLoss().to(self.device)
          
          loss = loss_f(y_pred, y.to(self.device))
          
          loss.backward()
          
          with torch.no_grad():
              self.W1 -= self.learning_rate * self.W1.grad.data
              self.W2 -= self.learning_rate * self.W2.grad.data
              self.W1.grad.data.zero_()
              self.W2.grad.data.zero_()
      if epoch % 100 == 0:
          self.learning_rate *= self.lr_decay
      self.loss_hist.append(loss)
      if epoch % 50 == 0:
          print(f'Epoch {epoch}, loss = {loss}')
  
  def save(self):
    torch.save(self.W1, self.layer1_params_file)
    torch.save(self.W2, self.layer2_params_file)
    print("Model parameters saved successfully!")
  
  def load(self):
    self.W1 = torch.load(self.layer1_params_file, map_location=torch.device(self.device))
    self.W2 = torch.load(self.layer2_params_file, map_location=torch.device(self.device))
    print("Model parameters loaded successfully!")

  def get_embedding(self, x):
    return self.get_input_tensor(x.to(self.device)).mm(self.W1)

"""*********************
Persian
"""

# persian_skip_gram = SkipGram(model_name="persian", embedding_dims=30, learning_rate=35, vocab_size=persian_vocab_size, num_epochs=10000, batch_size=len(persian_train_emb))

# print("Train Persian Skip-Gram:")
# persian_skip_gram.fit(persian_train_emb)
# persian_skip_gram.save()

persian_skip_test = SkipGram(model_name="persian", embedding_dims=30, vocab_size=persian_vocab_size, num_epochs=5000, batch_size=len(persian_train_emb))
persian_skip_test.load()

persian_embedding = torch.Tensor.cpu(persian_skip_test.get_embedding(torch.tensor(list(persian_vocabulary.values())))).detach().numpy()
# print(persian_embedding)
print(persian_embedding.shape)

# persian_skip_test.fit(persian_train_emb)
# persian_skip_test.save()

"""Persian
*********************

*********************
English
"""

# english_skip_gram = SkipGram(model_name="english", embedding_dims=30, learning_rate=10, vocab_size=persian_vocab_size, num_epochs=1000, batch_size=len(persian_train_emb))

# print("Train English Skip-Gram:")
# english_skip_gram.fit(english_train_emb)
# english_skip_gram.save()

english_skip_test = SkipGram(model_name="english", embedding_dims=30, vocab_size=persian_vocab_size, num_epochs=5000, batch_size=len(english_train_emb))
english_skip_test.load()

english_embedding = torch.Tensor.cpu(english_skip_test.get_embedding(torch.tensor(list(english_vocabulary.values())))).detach().numpy()
# print(english_embedding)
print(english_embedding.shape)

# english_skip_test.fit(english_train_emb)
# english_skip_test.save()

"""English
*********************
"""

# def get_corpus_embeddings(corpus, vocabulary, embedding):
#   return np.array([np.mean(embedding[word_idx], axis=0) for word_idx in
#           [[vocabulary[word] for word in sentence.split()] for sentence in corpus]])

true = 0
for word in range(len(persian_embedding)):
   similar = np.argmax(persian_embedding@(persian_embedding[word].T))
   true += similar==word

print(true/len(persian_embedding))

true = 0
for word in range(len(english_embedding)):
   similar = np.argmax(english_embedding@(english_embedding[word].T))
   true += similar==word

print(true/len(english_embedding))

"""-------------------------------------------------------------------------------"""

from sklearn.feature_extraction.text import TfidfVectorizer
persian_vectorizer = TfidfVectorizer(vocabulary=persian_vocabulary)
X_persian = persian_vectorizer.fit_transform(persian_corpus)
print(persian_vectorizer.get_feature_names_out().shape)
print(X_persian.shape)

english_vectorizer = TfidfVectorizer(vocabulary=english_vocabulary)
X_english = english_vectorizer.fit_transform(english_corpus)
print(english_vectorizer.get_feature_names_out().shape)
print(X_english.shape)
print(len(english_vocabulary))

def get_corpus_embeddings(X, corpus, vocabulary, embedding):
  corp_embedding = np.zeros((len(corpus) , 30))
  for sentence_idx in range(len(corpus)):
    sum_embedding = np.zeros(30)
    sum_tfidf = 0.0
    for word in corpus[sentence_idx].split():
      word_index = vocabulary[word]
      tfidf = X[sentence_idx, word_index]
      sum_tfidf += tfidf
      sum_embedding = sum_embedding + (embedding[word_index])*tfidf
    corp_embedding[sentence_idx] = sum_embedding / sum_tfidf
  return corp_embedding

persian_corpus_embeddings = get_corpus_embeddings(X_persian, persian_corpus, persian_vocabulary, persian_embedding)
english_corpus_embeddings = get_corpus_embeddings(X_english, english_corpus, english_vocabulary, english_embedding)

# print(persian_corpus_embeddings.shape)
# print(english_corpus_embeddings.shape)

# corp = "سندروس"
# # persian_corp_embeddings = np.array(persian_embedding[word_idx] for word_idx in
# #           [persian_vocabulary[word] for word in corp.split()])
# # persian_corp_embeddings = get_corpus_embeddings(corp, persian_vocabulary, persian_embedding)

# # print(persian_vocabulary['اسپري'])
# # print(persian_vocabulary['باريج'])
# # print(persian_vocabulary['اسانس'])
# # print(persian_embedding[persian_vocabulary['اسپري']])
# # print(persian_embedding[persian_vocabulary['باريج']])
# # print(persian_embedding[persian_vocabulary['اسانس']])
# # print(np.array(word_idx, axis=0) for word_idx in [persian_vocabulary[word] for word in corp.split()])
# # print(persian_embedding[11])
# # print(persian_embedding[40])
# # print(persian_embedding[161])
# print(len(corp.split()))
# embed = np.zeros((len(persian_embedding[0],)), dtype=int)
# for i in range(len(corp.split())):
#   embed = embed + persian_embedding[persian_vocabulary[corp.split()[i]]]
# embed = embed / len(corp.split())
# print(embed)
# # similarity1 = embed.dot(persian_embedding.T) 
# # print(np.argmax(similarity1))
# # print(similarity1)
# # print(similarity1.shape)

# similarity = embed.dot(persian_corpus_embeddings.T) 
# print(persian_corpus[np.argmax(similarity)])
# print(similarity.shape)

# [(persian_corpus[idx] , similarity[idx]) for idx in similarity.argsort()[-50:][::-1]]

# import pandas as pd 
# pd.DataFrame(persian_corpus_embeddings).to_csv("/content/drug_embedding30_persian.csv")
# pd.DataFrame(persian_corpus_embeddings).to_csv("/content/drug_embedding30_english.csv")

persian_corpus_embeddings = persian_corpus_embeddings / np.sqrt(np.sum(persian_corpus_embeddings**2, axis=1))[:,None]
print(np.sum(persian_corpus_embeddings**2, axis=1).shape)
print(np.sum(persian_corpus_embeddings**2, axis=1)[:10])
# squared_sum = np.sqrt(np.sum(persian_corpus_embeddings**2, axis=1))
# for idx, vector in enumerate(persian_corpus_embeddings):
#   persian_corpus_embeddings[idx] = vector / squared_sum[idx]
# print(np.sum(persian_corpus_embeddings**2, axis=1).shape)
# print(np.sum(persian_corpus_embeddings**2, axis=1)[:10])

english_corpus_embeddings = english_corpus_embeddings / np.sqrt(np.sum(english_corpus_embeddings**2, axis=1))[:,None]
print(np.sum(english_corpus_embeddings**2, axis=1).shape)
print(np.sum(english_corpus_embeddings**2, axis=1)[:10])

# Deep
class Translator():
  def __init__(self, source_embedding_dims, hidden_dims1, hidden_dims2, 
               destination_embedding_dims, model_name, num_epochs=5000, 
               batch_size=20, learning_rate=2e-1, lr_decay=0.99):
    self.device = torch.device('cuda')
    self.source_embedding_dims = source_embedding_dims
    self.destination_embedding_dims = destination_embedding_dims
    self.lr_decay = lr_decay
    self.hidden_dims = hidden_dims1
    initrange = 0.5 / source_embedding_dims
    self.model_name = model_name
    self.W1 = Variable(torch.randn(source_embedding_dims, hidden_dims1,
                                   device=self.device).uniform_(-initrange, initrange).double(),
                                   requires_grad=True)
    self.W2 = Variable(torch.randn(hidden_dims1, hidden_dims2,
                                   device=self.device).uniform_(-initrange, initrange).double(),
                                   requires_grad=True).double()
    self.W3 = Variable(torch.randn(hidden_dims2, destination_embedding_dims,
                                   device=self.device).uniform_(-initrange, initrange).double(),
                                   requires_grad=True).double()
    self.num_epochs = num_epochs
    self.batch_size = batch_size
    self.learning_rate = learning_rate
    self.activation = torch.nn.LeakyReLU(0.1)
    self.loss_hist = []
    self.layer1_params_file = f"{self.model_name}_translator_layer1.pt"
    self.layer2_params_file = f"{self.model_name}_translator_layer2.pt"
    self.layer3_params_file = f"{self.model_name}_translator_layer3.pt"
  
  def fit(self, source_corpus_embedding, destination_corpus_embedding):
    for epoch in range(self.num_epochs):
      for x,y in zip(DataLoader(source_corpus_embedding,
                                batch_size=self.batch_size),
                      DataLoader(destination_corpus_embedding,
                                 batch_size=self.batch_size)):
          h = x.to(self.device).double().mm(self.W1)
          h = self.activation(h).to(self.device)
          h = h.mm(self.W2).to(self.device)
          h = self.activation(h).to(self.device)
          y_pred = h.mm(self.W3).to(self.device)
          loss_f = torch.nn.MSELoss().to(self.device)
          loss = loss_f(y_pred, y.to(self.device))
          loss.backward()
          
          with torch.no_grad():
              self.W1 -= self.learning_rate * self.W1.grad.data
              self.W1.grad.data.zero_()
              self.W2 -= self.learning_rate * self.W2.grad.data
              self.W2.grad.data.zero_()
              self.W3 -= self.learning_rate * self.W3.grad.data
              self.W3.grad.data.zero_()
      self.loss_hist.append(loss)
      if epoch % 100 == 0:
          self.learning_rate *= self.lr_decay
      if epoch % 50 == 0:
          print(f'Epoch {epoch}, loss = {loss}')


  def save(self):
    torch.save(self.W1, self.layer1_params_file)
    torch.save(self.W2, self.layer2_params_file)
    torch.save(self.W3, self.layer3_params_file)
    print("Model parameters saved successfully!")
  
  def load(self):
    self.W1 = torch.load(self.layer1_params_file, map_location=torch.device(self.device))
    self.W2 = torch.load(self.layer2_params_file, map_location=torch.device(self.device))
    self.W3 = torch.load(self.layer3_params_file, map_location=torch.device(self.device))
    print("Model parameters loaded successfully!")
    
  def translate(self, x):
    return self.activation(self.activation(torch.tensor(x).to(self.device).matmul(self.W1)).matmul(self.W2)).matmul(self.W3)

##Wide
# class Translator():
#   def __init__(self, source_embedding_dims, hidden_dims, destination_embedding_dims, num_epochs=5000, batch_size=20, learning_rate=2e-1, lr_decay=0.99):
#     self.device = torch.device('cuda')
#     self.source_embedding_dims = source_embedding_dims
#     self.destination_embedding_dims = destination_embedding_dims
#     self.lr_decay = lr_decay
#     self.hidden_dims = hidden_dims
#     initrange = 0.5 / source_embedding_dims
#     self.W1 = Variable(torch.randn(source_embedding_dims, hidden_dims,
#                                    device=self.device).uniform_(-initrange,
#                                     initrange).double(), requires_grad=True)
#     self.W2 = Variable(torch.randn(hidden_dims, destination_embedding_dims,
#                                    device=self.device).uniform_(-initrange, 
#                                    initrange).double(), requires_grad=True)
#     self.num_epochs = num_epochs
#     self.batch_size = batch_size
#     self.learning_rate = learning_rate
#     self.activation = torch.nn.LeakyReLU(0.1)
#     self.loss_hist = []
#     self.layer1_params_file = "translator_layer1.pt"
#     self.layer2_params_file = "translator_layer2.pt"
  
#   def fit(self, source_corpus_embedding, destination_corpus_embedding):
#     for epoch in range(self.num_epochs):
#       for x,y in zip(DataLoader(source_corpus_embedding,
#                                 batch_size=self.batch_size),
#                       DataLoader(destination_corpus_embedding,
#                                  batch_size=self.batch_size)):
#           h = x.to(self.device).double().mm(self.W1)
#           h = self.activation(h).to(self.device)
#           y_pred = h.mm(self.W2).to(self.device)
#           loss_f = torch.nn.MSELoss().to(self.device)
#           loss = loss_f(y_pred, y.to(self.device))
#           loss.backward()
          
#           with torch.no_grad():
#               self.W1 -= self.learning_rate * self.W1.grad.data
#               self.W1.grad.data.zero_()
#               self.W2 -= self.learning_rate * self.W2.grad.data
#               self.W2.grad.data.zero_()
#       self.loss_hist.append(loss)
#       if epoch % 100 == 0:
#           self.learning_rate *= self.lr_decay
#       if epoch % 50 == 0:
#           print(f'Epoch {epoch}, loss = {loss}')


#   def save(self):
#     torch.save(self.W1, self.layer1_params_file)
#     torch.save(self.W2, self.layer2_params_file)
#     print("Model parameters saved successfully!")
  
#   def load(self):
#     self.W1 = torch.load(self.layer1_params_file, map_location=torch.device(self.device))
#     self.W2 = torch.load(self.layer2_params_file, map_location=torch.device(self.device))
#     print("Model parameters loaded successfully!")
    
#   def translate(self, x):
#     return self.activation(torch.tensor(x).to(self.device).matmul(self.W1)).matmul(self.W2)

# Deep
# persian to english
hidden_dims = 400
persian_vocab_size, persian_embedding_dims = persian_corpus_embeddings.shape
english_vocab_size, english_embedding_dims = english_corpus_embeddings.shape
persian_to_english_translator = Translator(persian_embedding_dims, hidden_dims,
                                           hidden_dims, english_embedding_dims,
                                           model_name="pte", num_epochs=8000,
                                           learning_rate=30, 
                                           batch_size=persian_vocab_size)

# Deep
#english to persian
hidden_dims = 512
persian_vocab_size, persian_embedding_dims = persian_corpus_embeddings.shape
english_vocab_size, english_embedding_dims = english_corpus_embeddings.shape
english_to_persian_translator = Translator(english_embedding_dims, hidden_dims,
                                           hidden_dims, persian_embedding_dims,
                                           model_name="etp", num_epochs=8000,
                                           learning_rate=30,
                                           batch_size=english_vocab_size)

# Wide
# hidden_dims = 2048
# persian_vocab_size, persian_embedding_dims = persian_corpus_embeddings.shape
# english_vocab_size, english_embedding_dims = english_corpus_embeddings.shape
# persian_to_english_translator = Translator(persian_embedding_dims, hidden_dims,
#                                            english_embedding_dims,
#                                            num_epochs=8000, learning_rate=30,
#                                            batch_size=persian_vocab_size)

# persian_to_english_translator.fit(persian_corpus_embeddings, english_corpus_embeddings)
# persian_to_english_translator.save()

# english_to_persian_translator.fit(english_corpus_embeddings, persian_corpus_embeddings)
# english_to_persian_translator.save()

# Deep
# persian to english
# persian_to_english_translator = Translator(persian_embedding_dims, hidden_dims,
#                                            hidden_dims, english_embedding_dims,
#                                            learning_rate=5, num_epochs=12000,
#                                            batch_size=persian_vocab_size)
# persian_to_english_translator.load()
# persian_to_english_translator.fit(persian_corpus_embeddings, english_corpus_embeddings)
# persian_to_english_translator.save()

# Deep
# english to persian
# english_to_persian_translator = Translator(english_embedding_dims, hidden_dims,
#                                            hidden_dims, persian_embedding_dims,
#                                            learning_rate=5, num_epochs=12000,
#                                            batch_size=english_vocab_size)
# english_to_persian_translator.load()
# english_to_persian_translator.fit(english_corpus_embeddings, persian_corpus_embeddings)
# english_to_persian_translator.save()

## Wide
# persian_to_english_translator = Translator(persian_embedding_dims, hidden_dims, english_embedding_dims, learning_rate=20, num_epochs=12000, batch_size=persian_vocab_size)
# persian_to_english_translator.load()
# persian_to_english_translator.fit(persian_corpus_embeddings, english_corpus_embeddings)
# persian_to_english_translator.save()

# persian_to_english_translator_test = Translator(persian_embedding_dims, 256, english_embedding_dims, batch_size=persian_vocab_size,num_epochs=30000)
# persian_to_english_translator_test.load()

english_to_persian_translator.load()
persian_to_english_translator.load()

# true = 0
# for index in range(len(persian_corpus)):
#   # print(persian_corpus[index])
#   translation = torch.Tensor.cpu(persian_to_english_translator.translate(persian_corpus_embeddings[index])).detach().numpy() 
#   similar = np.argmax(english_corpus_embeddings@translation.T)
#   true += (similar==index)
#   # print(english_corpus[similar])
#   # print("_" * 100)

# print("accuracy = " ,true/len(persian_corpus))

true = 0
for index in range(len(english_corpus)):
  # print(persian_corpus[index])
  translation = torch.Tensor.cpu(english_to_persian_translator.translate(english_corpus_embeddings[index])).detach().numpy() 
  similar = np.argmax(persian_corpus_embeddings @ translation.T)
  true += (similar==index)
  # print(english_corpus[similar])
  # print("_" * 100)

print(f"English to Persian Accuracy: ", true / len(english_corpus)*100, "%")

true = 0
for index in range(len(persian_corpus)):
  # print(persian_corpus[index])
  translation = torch.Tensor.cpu(persian_to_english_translator.translate(persian_corpus_embeddings[index])).detach().numpy() 
  similar = np.argmax(english_corpus_embeddings@translation.T)
  true += (similar==index)
  # print(english_corpus[similar])
  # print("_" * 100)

print("Persian to English Accuracy: ", true / len(persian_corpus)*100, "%")

"""****
Accuracy Calculation Using Cosine Similariy
"""

def cos_similarity(persian_embedding, english_embedding):
  sim = persian_embedding @ english_embedding.T # 3442 * 30 @ 30 * 1 = 1 * 1
  persian_sqrt = np.sqrt(persian_embedding @ persian_embedding.T) # 1 * 30 @ 30 * 1 = 1 * 1
  englsih_sqrt = np.sqrt(english_embedding @ english_embedding.T) # 3442 * 30 @ 30 * 3442 = 3442 * 3442
  return (sim / (persian_sqrt * englsih_sqrt))

true = 0
for index in range(len(english_corpus)):
  # print(persian_corpus[index])
  translation = torch.Tensor.cpu(english_to_persian_translator.translate(english_corpus_embeddings[index])).detach().numpy() 
  max = 0
  similar = -1
  for idx, persian_sentence in enumerate(persian_corpus_embeddings):
    similarity = cos_similarity(persian_sentence, translation)
    if similarity > max:
      max = similarity
      similar = idx
  true += (similar==index)
  # print(english_corpus[similar])
  # print("_" * 100)

print(f"English to Persian Accuracy: ", true / len(english_corpus)*100, "%")

true = 0
for index in range(len(persian_corpus)):
  # print(persian_corpus[index])
  translation = torch.Tensor.cpu(persian_to_english_translator.translate(persian_corpus_embeddings[index])).detach().numpy() 
  max = 0
  similar = -1
  for idx, english_sentence in enumerate(english_corpus_embeddings):
    similarity = cos_similarity(english_sentence, translation)
    if similarity > max:
      max = similarity
      similar = idx
  true += (similar==index)
  # print(english_corpus[similar])
  # print("_" * 100)

print(f"English to Persian Accuracy: ", true / len(persian_corpus)*100, "%")

"""*****"""

true = 0
for index in range(len(persian_corpus)):
  # print(persian_corpus[index])
  translation = torch.Tensor.cpu(persian_to_english_translator.translate(persian_corpus_embeddings[index])).detach().numpy() 
  similar = np.argmax(english_corpus_embeddings@translation.T)
  true += (similar==index)
  # print(english_corpus[similar])
  # print("_" * 100)

print("Persian to English Accuracy: ", true / len(persian_corpus)*100, "%")

import random
def calculate_samples_similarities(num_samples=5, translator="persian_to_english_translator"):
  indices = random.sample(range(0, len(persian_corpus)), num_samples)
  for index in indices:
      translation = torch.Tensor.cpu(persian_to_english_translator.translate(persian_corpus_embeddings[index])).detach().numpy()
      cos_sim = cos_similarity(translation , english_corpus_embeddings[index])
      print("دارو : ",persian_corpus[index])
      print("drug : ",english_corpus[index])
      print("شباهت کسینیوسی : ", cos_sim)
      print("_"*100)
calculate_samples_similarities()
calculate_samples_similarities(translator="english_to_persian_translator")

# english_to_persian_translator = Translator(persian_embedding_dims, english_embedding_dims, batch_size=2000, learning_rate=9e-1)
# english_to_persian_translator.fit(persian_embedding, english_embedding)

import time
language = input("Choose your language? : (persian = 1 , english = 2)")
drug_name = input("Enter drug name: ").lower()
language = int(language)
translator = "persian_to_english_translator" if language == 1 else "english_to_persian_translator"
corpus = "persian_corpus" if language == 1 else "english_corpus"
target_corpus = "persian_corpus" if language == 2 else "english_corpus"
# print("حق" in persian_corpus[0])
# for i in range(len(persian_corpus)):
#   if "حق" in persian_corpus[i]: 
#     print(persian_corpus[i])

st = time.time()
matches = []
# for i in range(len(persian_corpus)):
#   if drug_name in persian_corpus[i]: 
#     print(persian_corpus[i])
  # matches.append()
matches = [(sentence, idx, torch.Tensor.cpu(eval(translator).translate(
        eval(corpus+"_embeddings")[idx])).detach().numpy()) for idx, sentence
        in enumerate(eval(corpus)) if drug_name in sentence]
et = time.time()
print("_"*1000)
print(f"About {len(matches)} results ({et-st} seconds)") 
print("_"*1000)
pairs = {}
for translations in matches:
  sentence, index, translation = translations
  cos_sim = cos_similarity(translation , eval(target_corpus+"_embeddings")[index])
  # pairs["Similar drug name: " + sentence+"\n"+"Translation: "+eval(target_corpus)
  # [np.argmax(eval(target_corpus+"_embeddings") @ translation.T)]+"\n"+"شباهت کسینیوسی : "+ str(cos_sim)] = cos_sim
  
  print("Similar drug name: ", sentence)
  print("Translation: ", eval(target_corpus)[np.argmax(eval(target_corpus+"_embeddings") @ translation.T)])
  print("شباهت کسینیوسی : ", cos_sim)
  print("_"*100)

# for x in sorted(pairs):
#   print(x)
#   print("_"*100)